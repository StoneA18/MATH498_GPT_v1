{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487be7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17578062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model:int\n",
    "    d_vocab:int\n",
    "    d_hidden:int\n",
    "    max_seq_len:int\n",
    "    numTrans:int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(self.act(self.fc1(x)))\n",
    "        return x\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.Wqk = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "        self.Wov = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "\n",
    "        # mask = torch.triu(torch.ones(config.max_seq_len, config.max_seq_len),\n",
    "        #                   diagonal=1\n",
    "        #                   )\n",
    "        # mask = mask.masked_fill(mask==1, -float('inf'))\n",
    "        # self.register_buffer(\"M\", mask)\n",
    "\n",
    "    def get_mask(self, n):\n",
    "        mask = torch.triu(torch.ones(n, n), diagonal=1)\n",
    "        mask = mask.masked_fill(mask==1, -float('inf'))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x): # x -> \n",
    "        temp = x @ self.Wqk @ x.T + self.get_mask(x.shape(0))\n",
    "        scores = torch.softmax(temp, dim=1)\n",
    "\n",
    "        scores = scores @ x @ self.Wov\n",
    "\n",
    "        return scores\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.mlp(x) + self.attn(x) + x\n",
    "        return res\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.d_vocab, config.d_model)\n",
    "        self.tbs = nn.ModuleList([Transformer(config) for i in range(self.config.numTrans)])\n",
    "        #self.t1 = Transformer(config)\n",
    "    \n",
    "    def forward(self, x_tokens):\n",
    "        x = self.embedding(x_tokens)\n",
    "        temp = x\n",
    "        for i in range(self.config.numTrans):\n",
    "            temp = self.tbs[i](temp)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "36236859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4443,  1.4568,  0.3347, -0.3722, -1.4085, -1.2414, -0.6048,  0.0476,\n",
       "         -0.8957, -0.5576, -1.6263,  1.4795,  1.0423,  1.4978, -2.2204,  0.1678,\n",
       "         -0.2253,  0.0305, -1.3987, -1.0215,  0.7041,  0.9197,  0.7740,  0.7017,\n",
       "          0.4842,  2.2864, -0.1840,  1.1123,  1.2535,  1.1048],\n",
       "        [-0.3318,  0.7217, -1.9149,  0.0842, -0.8393,  0.7559, -2.2055, -0.9364,\n",
       "          1.4806,  0.4718,  0.4114,  0.7480, -0.5534,  0.5398,  0.6972, -0.1158,\n",
       "          0.5376, -0.9454,  0.0379,  0.8882, -0.4611,  1.3352,  0.4513, -0.4222,\n",
       "          1.9414,  0.1637, -0.3131, -0.4753,  0.3018,  2.4414],\n",
       "        [-0.2408, -0.5617, -0.6802,  0.5548, -0.1123, -1.4520,  0.7677, -0.5034,\n",
       "         -0.0312,  0.5439,  1.1484,  1.1314,  1.2265, -1.5124, -0.1712, -1.6632,\n",
       "          1.0540, -0.1720, -0.9873, -0.1587, -0.5318, -0.0286,  1.1374,  0.2203,\n",
       "         -2.0199, -2.1378,  0.4648,  2.5427, -0.6818, -0.4421]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test no. 1\n",
    "config = Config(d_model=30, d_vocab=100, d_hidden=128, max_seq_len=3, numTrans=3)\n",
    "model = LanguageModel(config)\n",
    "x = torch.tensor([1, 5, 24])\n",
    "res = model(x)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dictionary with each of the 1000 most common english words. Swap out the file with other .txt files that just have words if you want.\n",
    "def get_common_word_dict(f_name = 'texts/words1000.txt'):\n",
    "    word_dict = {}\n",
    "    with open(f_name,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        i = 0\n",
    "        for line in lines:\n",
    "            word_dict[line.strip()] = i\n",
    "            i+=1\n",
    "    print(f\"Created dictionary with {i} words.\")\n",
    "    return word_dict\n",
    "\n",
    "#get a 1d torch tensor of tokens from a sequence of words\n",
    "# if you give it an empty dictionary it will create one for you with the words from the sentence.\n",
    "def tokenize_sentence(sentence, dictionary={}):\n",
    "    #sentence_arr = re.split('-|\\\\. |, | |\\n', sentence) #split on any of these possible delimiters we may see\n",
    "    sentence_arr = re.split('\\\\. |, | |\\n|\\t', sentence) #split on any of these possible delimiters we may see\n",
    "    tokens = [-1 for _ in range(len(sentence_arr))]\n",
    "    if len(sentence_arr)>21: \n",
    "        print(sentence_arr[:20])\n",
    "    for i, word in enumerate(sentence_arr):\n",
    "        word = word.lower()\n",
    "        #get rid of non alphanumeric characters for now\n",
    "        # if not(word.isalnum()):\n",
    "        #     pattern = r'[^a-zA-Z0-9]' \n",
    "        #     replacement = ''\n",
    "        #     word = re.sub(pattern, replacement, word)\n",
    "        token = dictionary.get(word, -1)\n",
    "        # if we don't know this word, add it to dictionary\n",
    "        if token == -1:\n",
    "            token = len(dictionary)\n",
    "            dictionary[word] = token\n",
    "        tokens[i] = token\n",
    "    # make it a 1d tensor\n",
    "    tokens = torch.tensor(tokens)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_many_sentences(sentences, dictionary={}):\n",
    "    token_batches = [[] for _ in sentences]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        token_batches[i] = tokenize_sentence(sentence)\n",
    "\n",
    "def tokenize_file(f_name, dictionary = {}):\n",
    "    with open(f_name,'r',encoding='utf-8') as f:\n",
    "        tokens = tokenize_sentence(f.read(), dictionary=dictionary)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eadb9624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dictionary with 1000 words.\n",
      "Getting token for word 'language': 498\n",
      "---------\n",
      "Created dictionary with 9884 words.\n",
      "Sentence to tokenize: 'typically this is completely random, but\n",
      "sometimes it could be learned.'\n",
      "Tokenized sentence: tensor([3836,   11,    7, 2318, 1853,   42, 1724,   15,  206,   18, 9884])\n",
      "---------\n",
      "Tokenized sentence: tensor([0, 1, 2, 3, 4, 5, 6, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# example of using these functions\n",
    "my_dict = get_common_word_dict()\n",
    "word2test = 'language'\n",
    "print(f\"Getting token for word '{word2test}':\",my_dict[word2test]) #the course is MATH498: Large Language Modles and the 498th most common word is apparently language which is funny\n",
    "print(\"---------\")\n",
    "sentence = \"typically this is completely random, but\\nsometimes it could be learned.\" #excerpt from a lecture I was in when writing this\n",
    "my_dict = get_common_word_dict('texts/google-10000-english-usa-no-swears.txt') #use bigger dictionary\n",
    "print(f\"Sentence to tokenize: '{sentence}'\")\n",
    "tokens = tokenize_sentence(sentence, my_dict)\n",
    "print(f\"Tokenized sentence: {tokens}\")\n",
    "print(\"---------\")\n",
    "sentence = \"now we will tokenize a sentence without a dictionary\" #excerpt from a lecture I was in when writing this\n",
    "tokens = tokenize_sentence(sentence) #tokenize without a dictionary just assigns tokens to words\n",
    "print(f\"Tokenized sentence: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e8743bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dictionary with 9884 words.\n",
      "['Here', 'is', 'how', 'to', 'make', 'Miso-Butter', 'Roast', 'Chicken', 'With', 'Acorn', 'Squash', 'Panzanella', 'You', 'need', '1', '(3½–4-lb.)', 'whole', 'chicken', '2¾', 'tsp']\n",
      "tensor([  69,    7,   86,    3,  131, 9884, 9885, 3570,   12, 9886, 9887, 9888,\n",
      "          14,  181, 9889, 9890,  936, 3570, 9891, 9892])\n"
     ]
    }
   ],
   "source": [
    "my_dict = get_common_word_dict('texts/google-10000-english-usa-no-swears.txt')\n",
    "tokens = tokenize_file('texts/recipes.txt', dictionary=my_dict)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9fc48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test no. 2 (run after cell above)\n",
    "from processing import get_recipe_arr\n",
    "\n",
    "recipes = get_recipe_arr()\n",
    "# config = Config(d_model=30, d_vocab=len(my_dict), d_hidden=128, max_seq_len=len(tokens), numTrans=3)\n",
    "# model = LanguageModel(config)\n",
    "# res = model(tokens)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40273159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
