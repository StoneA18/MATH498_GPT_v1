{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model:int\n",
    "    d_vocab:int\n",
    "    d_hidden:int\n",
    "    max_seq_len:int\n",
    "    numTrans:int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(self.act(self.fc1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.Wqk = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "        self.Wov = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "\n",
    "        mask = torch.triu(torch.ones(config.max_seq_len, config.max_seq_len),\n",
    "                          diagonal=1\n",
    "                          )\n",
    "        mask = mask.masked_fill(mask==1, -float('inf'))\n",
    "        self.register_buffer(\"M\", mask)\n",
    "\n",
    "    \n",
    "    def forward(self, x): \n",
    "        T = x.size(0)\n",
    "        temp = x @ self.Wqk @ x.T + self.M[:T, :T]\n",
    "        scores = torch.softmax(temp,dim=-1)\n",
    "        scores = scores @ x @ self.Wov\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(config)\n",
    "        self.mlp = MLP(config)\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #res = self.mlp(x) + self.attn(x) + x\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out = self.attn(x_norm)\n",
    "        x = x+attn_out\n",
    "        x_norm = self.ln2(x)\n",
    "        mlp_out = self.mlp(x_norm)\n",
    "        x = x+mlp_out\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.d_vocab, config.d_model)\n",
    "        self.tbs = nn.ModuleList([Transformer(config) for i in range(config.numTrans)])\n",
    "        self.lm_head = nn.Linear(config.d_model, config.d_vocab)\n",
    "        #self.t1 = Transformer(config)\n",
    "    \n",
    "    def forward(self, x_tokens):\n",
    "        x = self.embedding(x_tokens)\n",
    "        #print(\"print:\", x)\n",
    "        #print(x.shape)\n",
    "        #x = self.tbs[0](x)\n",
    "        temp = x\n",
    "        for i in range(self.config.numTrans):\n",
    "            temp = self.tbs[i](temp)\n",
    "\n",
    "        #X = torch.stack(x)\n",
    "        logits = self.lm_head(temp)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(d_model=30, d_vocab=100, d_hidden=128, max_seq_len=3, numTrans=3)\n",
    "model = LanguageModel(config)\n",
    "x = torch.tensor([1, 5, 24])\n",
    "#print(x)\n",
    "res = model(x)\n",
    "#print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(10, 10),\n",
    "                          diagonal=1\n",
    "                          )\n",
    "print(mask)\n",
    "mask = mask.masked_fill(mask==1, -float('inf'))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\" # Just a demo dataset, we can think about more creative datasets.\n",
    "r = requests.get(url)\n",
    "text = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_recipes_text' from 'processing' (c:\\Users\\amsba\\Desktop\\Coding\\Class\\LLMs\\a1_transformer\\processing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_recipes_text\n\u001b[32m      2\u001b[39m text = get_recipes_text()\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'get_recipes_text' from 'processing' (c:\\Users\\amsba\\Desktop\\Coding\\Class\\LLMs\\a1_transformer\\processing.py)"
     ]
    }
   ],
   "source": [
    "from processing import get_recipes_text\n",
    "text = get_recipes_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7030\n"
     ]
    }
   ],
   "source": [
    "text = text.lower().replace(\"\\n\", \" \")\n",
    "tokens = text.split()\n",
    "tokens = re.findall(r\"\\b\\w+\\b\", text.lower()) # I just used normal tokenization like word level token (every word is a token). We need to think about more advance tokenization techniques\n",
    "\n",
    "\n",
    "\n",
    "vocab = list(set(tokens))\n",
    "vocab.sort()\n",
    "\n",
    "token2id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id2token = {idx: tok for tok, idx in token2id.items()}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128769\n"
     ]
    }
   ],
   "source": [
    "config = Config(d_model=64, d_vocab=len(vocab), d_hidden=128, max_seq_len=1024, numTrans=2)  \n",
    "\n",
    "token_ids = [token2id[tok] for tok in tokens]\n",
    "\n",
    "print(len(token_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 64])\n",
      "tensor([[-0.3203,  0.5543, -0.0435,  ...,  1.1255,  1.8135, -0.7516],\n",
      "        [ 0.5108, -0.7017,  1.3207,  ..., -0.9895, -0.5102,  1.1979],\n",
      "        [-1.5709,  0.4920,  0.8093,  ..., -1.2583,  0.8982,  0.8086],\n",
      "        ...,\n",
      "        [-0.8477,  0.7623, -0.1941,  ...,  0.0026,  1.6839, -1.6065],\n",
      "        [ 0.2184, -0.3358, -1.8773,  ..., -0.9616,  0.3345,  1.2210],\n",
      "        [-0.4483,  1.4829,  0.8466,  ..., -0.4188,  0.2324, -0.5937]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing....\n",
    "embedding = nn.Embedding(num_embeddings=config.d_vocab, embedding_dim=config.d_model)\n",
    "\n",
    "token_ids_tensor = torch.tensor(token_ids[:config.max_seq_len])\n",
    "#print(token_ids_tensor)\n",
    "\n",
    "x = embedding(token_ids_tensor)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "x_ids = torch.tensor(token_ids[:config.max_seq_len])\n",
    "y_ids = torch.tensor(token_ids[1:config.max_seq_len+1])\n",
    "\n",
    "print(x_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7186, -0.1856,  1.7579,  ...,  0.4096, -0.7695,  1.5261],\n",
       "        [-1.3910,  0.8565,  1.6679,  ..., -0.1610, -2.6980,  3.8181],\n",
       "        [-1.9061,  0.6609,  1.6998,  ...,  0.5690, -0.5054,  2.8198],\n",
       "        ...,\n",
       "        [ 1.0073,  0.1663,  1.3406,  ...,  0.1250, -0.6013,  3.8738],\n",
       "        [-1.4835, -1.8537, -0.6857,  ..., -3.0837, -0.1971,  0.5632],\n",
       "        [-2.4108, -0.0838, -0.6624,  ...,  0.4469, -1.1086,  2.2518]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(config)\n",
    "logits = model(x_ids)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 7030])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)\n",
    "print(y_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "targets = y_ids\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  10.669713020324707\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, targets)\n",
    "print(\"Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 10.4195\n",
      "step 50, loss = 7.2960\n",
      "step 100, loss = 6.4542\n",
      "step 150, loss = 6.2343\n",
      "step 200, loss = 6.2475\n",
      "step 250, loss = 6.0763\n",
      "step 300, loss = 6.1092\n",
      "step 350, loss = 5.9576\n",
      "step 400, loss = 5.8933\n",
      "step 450, loss = 5.9033\n",
      "step 500, loss = 5.9237\n",
      "step 550, loss = 5.6171\n",
      "step 600, loss = 5.2216\n",
      "step 650, loss = 5.4718\n",
      "step 700, loss = 5.5188\n",
      "step 750, loss = 5.7507\n",
      "step 800, loss = 5.8355\n",
      "step 850, loss = 5.5377\n",
      "step 900, loss = 5.5768\n",
      "step 950, loss = 5.9387\n"
     ]
    }
   ],
   "source": [
    "### Training Loop ###\n",
    "model = LanguageModel(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(1000):  # number of training steps\n",
    "    # sample a random chunk of text\n",
    "    start = np.random.randint(0, len(token_ids) - config.max_seq_len - 1)\n",
    "    x_ids = torch.tensor(token_ids[start:start+config.max_seq_len])\n",
    "    y_ids = torch.tensor(token_ids[start+1:start+config.max_seq_len+1])\n",
    "    logits = model(x_ids)\n",
    "    targets = y_ids\n",
    "    loss = loss_fn(logits, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"step {step}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing import get_recipe_arr, get_vocab\n",
    "\n",
    "recipe_arr = get_recipe_arr()\n",
    "\n",
    "vocab, token2id, id2token = get_vocab()\n",
    "\n",
    "config = Config(d_model=64, d_vocab=len(vocab), d_hidden=128, max_seq_len=1024, numTrans=2)  \n",
    "\n",
    "for recipe in recipe_arr:\n",
    "\n",
    "    model = LanguageModel(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for step in range(1000):  # number of training steps\n",
    "        # sample a random chunk of text\n",
    "        start = np.random.randint(0, len(token_ids) - config.max_seq_len - 1)\n",
    "        x_ids = torch.tensor(token_ids[start:start+config.max_seq_len])\n",
    "        y_ids = torch.tensor(token_ids[start+1:start+config.max_seq_len+1])\n",
    "        logits = model(x_ids)\n",
    "        targets = y_ids\n",
    "        loss = loss_fn(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"step {step}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is how to make Chinese Five-Spice Steak with Oranges and Sesame Broccolini. You need 3 small oranges (about 1 lb.), 1 1/2 tsp. Chinese five-spice powder, 1 tsp. light brown sugar, 2 1/4 tsp. kosher salt, divided, 1 hanger steak (about 1 1/4 lb.), cut in half lengthwise, center gristle removed, 2 Tbsp. vegetable oil, 3 bunches broccolini (about 1 1/2 lb.), trimmed, halved lengthwise if large, 2 Tbsp. toasted sesame oil, 1/2 tsp. crushed red pepper flakes, 1 tsp. toasted sesame seeds, plus more for serving, 3 scallions, thinly sliced, Flaky sea salt, Steamed rice and hot sauce (for serving; optional). Finely grate 2 tsp. orange zest from 1 orange into a small bowl. Cut all oranges in half; set aside.\\nAdd five-spice powder, brown sugar, and 2 tsp. kosher salt to bowl with zest and stir to combine. Rub steak all over with spice mixture.\\nHeat vegetable oil in a large heavy skillet (preferably cast iron) over high. Cook steak, turning often, until browned on all sides and an instant-read thermometer inserted into the center registers 130˚F for medium-rare, 15–20 minutes total. After steak has cooked for about 5 minutes, nestle orange halves cut side down around steak and cook until well browned, about 5 minutes. Transfer oranges to a plate. Transfer steak to a cutting board and let rest 10 minutes before slicing.\\nArrange broccolini in an even layer in same skillet and heat over high. Cook, undisturbed, until well charred, about 3 minutes. Toss and continue to cook until tender and slightly charred all over, about 3 minutes more. Transfer to a large bowl. Squeeze in juice from 2 orange halves over. Add sesame oil, red pepper flakes, 1 tsp. sesame seeds, and remaining 1/4 tsp. kosher salt and toss to coat.\\nTransfer steak to a platter. Arrange broccolini mixture alongside. Top with scallions, sea salt, and remaining sesame seeds. Serve with charred orange halves to squeeze over, rice, and hot sauce alongside, if desired.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allen not be of the house had been a great of the house was to the whole to the whole to the whole to the whole to the whole to the whole to the whole to the whole to the whole to the whole to the whole to the whole "
     ]
    }
   ],
   "source": [
    "max_num_tokens = 50\n",
    "prompt_text = \"But the greatness of Mr Collins could not have been so satisfactorily\"\n",
    "\n",
    "for i in range(max_num_tokens):\n",
    "    prompt_tokens = [token2id[tok] for tok in prompt_text.lower().split()]\n",
    "    prompt_tensor = torch.tensor(prompt_tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(prompt_tensor)\n",
    "    \n",
    "    last_logits = logits[-1]\n",
    "    prob = torch.softmax(last_logits, dim=-1)\n",
    "    next_token_id = torch.argmax(prob).item()\n",
    "    next_token = id2token[next_token_id]\n",
    "    print(next_token, end=' ')\n",
    "\n",
    "    # append to prompt\n",
    "    prompt_text += \" \" + next_token\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
