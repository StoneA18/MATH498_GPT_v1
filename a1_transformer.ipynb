{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487be7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17578062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model:int\n",
    "    d_vocab:int\n",
    "    d_hidden:int\n",
    "    max_seq_len:int\n",
    "    numTrans:int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(self.act(self.fc1(x)))\n",
    "        return x\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.Wqk = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "        self.Wov = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "\n",
    "        # mask = torch.triu(torch.ones(config.max_seq_len, config.max_seq_len),\n",
    "        #                   diagonal=1\n",
    "        #                   )\n",
    "        # mask = mask.masked_fill(mask==1, -float('inf'))\n",
    "        # self.register_buffer(\"M\", mask)\n",
    "\n",
    "    def get_mask(self, n):\n",
    "        mask = torch.triu(torch.ones(n, n), diagonal=1)\n",
    "        mask = mask.masked_fill(mask==1, -float('inf'))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x): # x -> \n",
    "        temp = x @ self.Wqk @ x.T + self.get_mask(x.shape(0))\n",
    "        scores = torch.softmax(temp, dim=1)\n",
    "\n",
    "        scores = scores @ x @ self.Wov\n",
    "\n",
    "        return scores\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.mlp(x) + self.attn(x) + x\n",
    "        return res\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.d_vocab, config.d_model)\n",
    "        self.tbs = nn.ModuleList([Transformer(config) for i in range(self.config.numTrans)])\n",
    "        #self.t1 = Transformer(config)\n",
    "    \n",
    "    def forward(self, x_tokens):\n",
    "        x = self.embedding(x_tokens)\n",
    "        temp = x\n",
    "        for i in range(self.config.numTrans):\n",
    "            temp = self.tbs[i](temp)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "36236859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6401, -1.4650, -1.9804,  0.0181,  1.4377, -0.5540,  0.8334,  1.0625,\n",
       "          1.2128,  0.1425, -0.9594, -1.2020, -1.5157,  0.6969,  0.7953,  0.8824,\n",
       "         -0.1013, -2.4711, -0.3805,  1.1801, -1.8117,  1.6826, -0.6585, -0.5494,\n",
       "          0.1886, -0.6855,  0.6655,  0.8366, -0.2649, -0.2142],\n",
       "        [ 1.4120, -0.3521, -0.1997, -1.7244,  0.2252, -1.2857,  1.8367, -0.8410,\n",
       "         -1.1282, -0.1868, -0.0222, -2.0352, -0.2730,  0.0104,  2.2856,  0.0565,\n",
       "          0.9172,  1.5861, -0.4419, -2.1909, -0.3763,  0.8004,  1.4102, -0.6706,\n",
       "          0.7833, -0.4972,  0.3585,  1.5416, -0.7342, -0.9873],\n",
       "        [-1.3552, -0.9058,  0.4763, -0.4191,  0.3843, -0.2799,  0.4363, -0.0388,\n",
       "          2.1658,  0.5358, -1.2588, -2.1171, -1.7697,  1.6070,  0.3294,  1.7726,\n",
       "         -0.3078, -1.2329,  1.0938, -1.2468,  1.8754,  0.6325, -1.3610, -0.9587,\n",
       "         -0.4842,  1.0510,  0.0856,  1.0140, -0.0437,  0.0885]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test no. 1\n",
    "config = Config(d_model=30, d_vocab=100, d_hidden=128, max_seq_len=3, numTrans=3)\n",
    "model = LanguageModel(config)\n",
    "x = torch.tensor([1, 5, 24])\n",
    "res = model(x)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a5db20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dictionary with each of the 1000 most common english words. Swap out the file with other .txt files that just have words if you want.\n",
    "def get_common_word_dict(f_name = 'texts/words1000.txt'):\n",
    "    word_dict = {}\n",
    "    with open(f_name,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        i = 0\n",
    "        for line in lines:\n",
    "            word_dict[line.strip()] = i\n",
    "            i+=1\n",
    "    print(f\"Created dictionary with {i} words.\")\n",
    "    return word_dict\n",
    "\n",
    "#get a 1d torch tensor of tokens from a sequence of words\n",
    "# if you give it an empty dictionary it will create one for you with the words from the sentence.\n",
    "def tokenize_sentence(sentence, dictionary={}):\n",
    "    #sentence_arr = re.split('-|\\\\. |, | |\\n', sentence) #split on any of these possible delimiters we may see\n",
    "    sentence_arr = re.split('\\\\. |, | |\\n|\\t', sentence) #split on any of these possible delimiters we may see\n",
    "    tokens = [-1 for _ in range(len(sentence_arr))]\n",
    "    if len(sentence_arr)>21: \n",
    "        print(sentence_arr[:20])\n",
    "    for i, word in enumerate(sentence_arr):\n",
    "        word = word.lower()\n",
    "        #get rid of non alphanumeric characters for now\n",
    "        # if not(word.isalnum()):\n",
    "        #     pattern = r'[^a-zA-Z0-9]' \n",
    "        #     replacement = ''\n",
    "        #     word = re.sub(pattern, replacement, word)\n",
    "        token = dictionary.get(word, -1)\n",
    "        # if we don't know this word, add it to dictionary\n",
    "        if token == -1:\n",
    "            token = len(dictionary)\n",
    "            dictionary[word] = token\n",
    "        tokens[i] = token\n",
    "    # make it a 1d tensor\n",
    "    tokens = torch.tensor(tokens)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_file(f_name, dictionary = {}):\n",
    "    with open(f_name,'r',encoding='utf-8') as f:\n",
    "        tokens = tokenize_sentence(f.read(), dictionary=dictionary)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eadb9624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dictionary with 1000 words.\n",
      "Getting token for word 'language': 498\n",
      "---------\n",
      "Created dictionary with 9884 words.\n",
      "Sentence to tokenize: 'typically this is completely random, but\n",
      "sometimes it could be learned.'\n",
      "Tokenized sentence: tensor([3836,   11,    7, 2318, 1853,   42, 1724,   15,  206,   18, 9884])\n",
      "---------\n",
      "Tokenized sentence: tensor([0, 1, 2, 3, 4, 5, 6, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# example of using these functions\n",
    "my_dict = get_common_word_dict()\n",
    "word2test = 'language'\n",
    "print(f\"Getting token for word '{word2test}':\",my_dict[word2test]) #the course is MATH498: Large Language Modles and the 498th most common word is apparently language which is funny\n",
    "print(\"---------\")\n",
    "sentence = \"typically this is completely random, but\\nsometimes it could be learned.\" #excerpt from a lecture I was in when writing this\n",
    "my_dict = get_common_word_dict('texts/google-10000-english-usa-no-swears.txt') #use bigger dictionary\n",
    "print(f\"Sentence to tokenize: '{sentence}'\")\n",
    "tokens = tokenize_sentence(sentence, my_dict)\n",
    "print(f\"Tokenized sentence: {tokens}\")\n",
    "print(\"---------\")\n",
    "sentence = \"now we will tokenize a sentence without a dictionary\" #excerpt from a lecture I was in when writing this\n",
    "tokens = tokenize_sentence(sentence) #tokenize without a dictionary just assigns tokens to words\n",
    "print(f\"Tokenized sentence: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e8743bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dictionary with 9884 words.\n",
      "['Here', 'is', 'how', 'to', 'make', 'Miso-Butter', 'Roast', 'Chicken', 'With', 'Acorn', 'Squash', 'Panzanella', 'You', 'need', '1', '(3½–4-lb.)', 'whole', 'chicken', '2¾', 'tsp']\n",
      "tensor([  69,    7,   86,    3,  131, 9884, 9885, 3570,   12, 9886, 9887, 9888,\n",
      "          14,  181, 9889, 9890,  936, 3570, 9891, 9892])\n"
     ]
    }
   ],
   "source": [
    "my_dict = get_common_word_dict('texts/google-10000-english-usa-no-swears.txt')\n",
    "tokens = tokenize_file('texts/recipes.txt', dictionary=my_dict)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9fc48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test no. 2 (run after cell above)\n",
    "from processing import get_recipe_arr\n",
    "\n",
    "recipes = get_recipe_arr()\n",
    "# config = Config(d_model=30, d_vocab=len(my_dict), d_hidden=128, max_seq_len=len(tokens), numTrans=3)\n",
    "# model = LanguageModel(config)\n",
    "# res = model(tokens)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40273159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
