{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487be7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17578062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model:int\n",
    "    d_vocab:int\n",
    "    d_hidden:int\n",
    "    max_seq_len:int\n",
    "    numTrans:int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04d2177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(self.act(self.fc1(x)))\n",
    "        return x\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.Wqk = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "        self.Wov = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "\n",
    "        mask = torch.triu(torch.ones(config.max_seq_len, config.max_seq_len),\n",
    "                          diagonal=1\n",
    "                          )\n",
    "        mask = mask.masked_fill(mask==1, -float('inf'))\n",
    "        self.register_buffer(\"M\", mask)\n",
    "\n",
    "    \n",
    "    def forward(self, x): # x -> \n",
    "        temp = x @ self.Wqk @ x.T + self.M\n",
    "        scores = torch.softmax(temp, dim=1)\n",
    "\n",
    "        scores = scores @ x @ self.Wov\n",
    "\n",
    "        return scores\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.mlp(x) + self.attn(x) + x\n",
    "        return res\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.d_vocab, config.d_model)\n",
    "        self.tbs = nn.ModuleList([Transformer(config) for i in range(self.config.numTrans)])\n",
    "        #self.t1 = Transformer(config)\n",
    "    \n",
    "    def forward(self, x_tokens):\n",
    "        x = self.embedding(x_tokens)\n",
    "        temp = x\n",
    "        for i in range(self.config.numTrans):\n",
    "            temp = self.tbs[i](temp)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36236859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7790,  1.5111,  2.3221,  0.1331, -0.8261, -1.4524,  0.7142, -1.4659,\n",
       "          0.9313, -0.1000,  0.6359,  0.8525,  0.6081,  0.5770,  0.2889, -0.9851,\n",
       "         -1.3726,  0.0752, -0.1806, -0.0041, -0.1597, -1.7332, -0.1296, -1.3163,\n",
       "         -0.6357,  0.0433,  0.1697,  0.9189,  0.9327, -0.3153],\n",
       "        [ 1.0340,  0.1765, -1.4960,  2.4389,  0.9019, -0.8978,  1.2643,  0.6570,\n",
       "         -0.5607,  0.5066, -0.5913,  0.3909, -0.7334,  2.0067,  0.1153, -1.5227,\n",
       "         -0.8420, -0.0774, -1.3043, -0.1377, -0.4756, -0.6318,  0.5207, -0.3547,\n",
       "          2.8538, -1.2115, -0.5458, -0.6359, -1.4277,  1.4784],\n",
       "        [-1.2599, -0.0744,  2.0473, -3.0340,  0.5045,  0.3664,  0.3595, -1.5179,\n",
       "         -0.5828,  0.5507,  0.0972,  0.6533,  0.7986,  0.9087, -0.1520, -0.0331,\n",
       "         -1.2830,  0.9694,  0.7137, -0.3809,  0.4446,  1.5697,  0.4052,  0.6522,\n",
       "          0.9760, -0.5865, -0.5780,  0.7453, -0.6120,  0.9173]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test no. 1\n",
    "config = Config(d_model=30, d_vocab=100, d_hidden=128, max_seq_len=3, numTrans=3)\n",
    "model = LanguageModel(config)\n",
    "x = torch.tensor([1, 5, 24])\n",
    "res = model(x)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a5db20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dictionary with each of the 1000 most common english words. Swap out the file with other .txt files that just have words if you want.\n",
    "def get_common_word_dict(f_name = 'texts/words1000.txt'):\n",
    "    word_dict = {}\n",
    "    with open(f_name,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        i = 0\n",
    "        for line in lines:\n",
    "            word_dict[line.strip()] = i\n",
    "            i+=1\n",
    "    print(f\"Created dictionary with {i} words.\")\n",
    "    return word_dict\n",
    "\n",
    "#get a 1d torch tensor of tokens from a sequence of words\n",
    "# if you give it an empty dictionary it will create one for you with the words from the sentence.\n",
    "def tokenize_sentence(sentence, dictionary={}):\n",
    "    sentence_arr = re.split('-|\\\\. |, | |\\n', sentence) #split on any of these possible delimiters we may see\n",
    "    tokens = [-1 for _ in range(len(sentence_arr))]\n",
    "    print(sentence_arr)\n",
    "    for i, word in enumerate(sentence_arr):\n",
    "        word = word.lower()\n",
    "        #get rid of non alphanumeric characters for now\n",
    "        if not(word.isalnum()):\n",
    "            pattern = r'[^a-zA-Z0-9]' \n",
    "            replacement = ''\n",
    "            word = re.sub(pattern, replacement, word)\n",
    "        token = dictionary.get(word, -1)\n",
    "        # if we don't know this word, add it to dictionary\n",
    "        if token == -1:\n",
    "            token = len(dictionary)\n",
    "            dictionary[word] = token\n",
    "        tokens[i] = token\n",
    "    # make it a 1d tensor\n",
    "    tokens = torch.tensor(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eadb9624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dictionary with 1000 words.\n",
      "Getting token for word 'language': 498\n",
      "---------\n",
      "Created dictionary with 9884 words.\n",
      "Sentence to tokenize: 'typically this is completely random, but\n",
      "sometimes it could be learned.'\n",
      "['typically', 'this', 'is', 'completely', 'random', 'but', 'sometimes', 'it', 'could', 'be', 'learned.']\n",
      "Tokenized sentence: tensor([3836,   11,    7, 2318, 1853,   42, 1724,   15,  206,   18, 3264])\n",
      "---------\n",
      "['now', 'we', 'will', 'tokenize', 'a', 'sentence', 'without', 'a', 'dictionary']\n",
      "Tokenized sentence: tensor([0, 1, 2, 3, 4, 5, 6, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# example of using these functions\n",
    "my_dict = get_common_word_dict()\n",
    "word2test = 'language'\n",
    "print(f\"Getting token for word '{word2test}':\",my_dict[word2test]) #the course is MATH498: Large Language Modles and the 498th most common word is apparently language which is funny\n",
    "print(\"---------\")\n",
    "sentence = \"typically this is completely random, but\\nsometimes it could be learned.\" #excerpt from a lecture I was in when writing this\n",
    "my_dict = get_common_word_dict('texts/google-10000-english-usa-no-swears.txt') #use bigger dictionary\n",
    "print(f\"Sentence to tokenize: '{sentence}'\")\n",
    "tokens = tokenize_sentence(sentence, my_dict)\n",
    "print(f\"Tokenized sentence: {tokens}\")\n",
    "print(\"---------\")\n",
    "sentence = \"now we will tokenize a sentence without a dictionary\" #excerpt from a lecture I was in when writing this\n",
    "tokens = tokenize_sentence(sentence) #tokenize without a dictionary just assigns tokens to words\n",
    "print(f\"Tokenized sentence: {tokens}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
