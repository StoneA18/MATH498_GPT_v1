{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f645e1fc",
   "metadata": {},
   "source": [
    "<h1>This is the primary notebook for the generative pre-trained transformer.</h1>\n",
    "<br>\n",
    "This file contains our cleaned, revised work, and experimental or older code should be put elsewhere\n",
    "\n",
    "**Authors:** Jesan Ahammed Ovi, Stone Amsbaugh\n",
    "\n",
    "**Instructor:** Michael Ivanitsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3282bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the necessary imports and define our config class\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "\n",
    "#our custom utility files\n",
    "from vocab_utility import get_token_arr, get_dictionaries\n",
    "from recipe_utility import *\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model:int\n",
    "    d_vocab:int\n",
    "    d_hidden:int\n",
    "    max_seq_len:int\n",
    "    n_transformers:int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b15b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell defines our MLP, Attention head, transformer that combines these, as well as the language model containing these\n",
    "\n",
    "#Multi layer perceptron module, just a NN\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(self.act(self.fc1(x)))\n",
    "        return x\n",
    "    \n",
    "#'secret sauce' attention head. Allows the model to look back at previous tokens indefinitely, and select what is important\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        #initialize our parameters to be random\n",
    "        self.Wqk = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "        self.Wov = nn.Parameter(torch.rand(config.d_model, config.d_model))\n",
    "\n",
    "        #create the mask, which isn't a model parameter but we still need it\n",
    "        mask = torch.triu(torch.ones(config.max_seq_len, config.max_seq_len), diagonal=1)\n",
    "        mask = mask.masked_fill(mask==1, -float('inf'))\n",
    "        self.register_buffer(\"M\", mask)\n",
    "\n",
    "    \n",
    "    def forward(self, x): \n",
    "        T = x.size(0)\n",
    "        temp = x @ self.Wqk @ x.T + self.M[:T, :T]\n",
    "        scores = torch.softmax(temp,dim=-1)\n",
    "        scores = scores @ x @ self.Wov\n",
    "\n",
    "        return scores\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(config)\n",
    "        self.mlp = MLP(config)\n",
    "        self.mlp_normalizer = nn.LayerNorm(config.d_model)\n",
    "        self.attn_normalizer = nn.LayerNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out = self.attn(self.attn_normalizer(x))\n",
    "        mlp_out = self.mlp(self.mlp_normalizer(x))\n",
    "\n",
    "        return x+attn_out+mlp_out\n",
    "    \n",
    "#compile multiple transformers, embedding layer, and our output layer, as well as our overall configurations into the language model\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(self.config.d_vocab, self.config.d_model)\n",
    "        self.tbs = nn.ModuleList([Transformer(self.config) for _ in range(self.config.n_transformers)])\n",
    "        self.lm_head = nn.Linear(self.config.d_model, self.config.d_vocab)\n",
    "    \n",
    "    def forward(self, x_tokens):\n",
    "        temp = self.embedding(x_tokens)\n",
    "        #look that propagates this through the transformer layers\n",
    "        for i in range(self.config.n_transformers):\n",
    "            temp = self.tbs[i](temp)\n",
    "\n",
    "        logits = self.lm_head(temp)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc91b7a",
   "metadata": {},
   "source": [
    "Now we have our architecture built out, we will implement a training loop. This first demonstration will be on the Gutenberg sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75ca0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some utility functions for processing vocabulary\n",
    "def get_dictionaries(tokens):\n",
    "    # takes array of words(tokens), makes forward and backward dictionaries from words to their token identifiers\n",
    "    forward_dict = {} #get token ID\n",
    "    backward_dict = {} #get english token\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        if token in forward_dict:\n",
    "            continue\n",
    "        #if new token, give it an ID\n",
    "        forward_dict[token] = i\n",
    "        backward_dict[i] = token\n",
    "        i+=1\n",
    "    \n",
    "    return forward_dict, backward_dict\n",
    "\n",
    "def get_token_arr(text):\n",
    "    #return([\"A\"])\n",
    "    #takes text and makes more standardized tokens\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\.? \\n]', '', text)\n",
    "    #add a space to make the punctuation their own tokens\n",
    "    text.replace(\".\",\" .\")\n",
    "    text.replace(\",\",\" ,\")\n",
    "    text.replace(\"?\",\" ?\")\n",
    "    text.replace(\"!\",\" !\")\n",
    "    \n",
    "    token_arr = text.split()\n",
    "    return token_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c216811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1 - Gutenberg - gather text\n",
    "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\" \n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07884c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tokens, token IDs\n",
    "tokens = get_token_arr(r.text) # TODO: Find a better way to tokenize this. Michael suggested we treat standard punctuation as their own tokens, split on whitespace, and drop other characters\n",
    "\n",
    "token_to_id, id_to_token = get_dictionaries(tokens) #get unique IDs for all the tokens\n",
    "d_vocab = len(token_to_id)\n",
    "\n",
    "config = Config(d_model=64, d_vocab=d_vocab, d_hidden=128, max_seq_len=1024, n_transformers=2)  \n",
    "\n",
    "token_ids = [token_to_id[tok] for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76049029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 10.7569\n",
      "step 50, loss = 7.6163\n",
      "step 100, loss = 6.7221\n",
      "step 150, loss = 6.2426\n",
      "step 200, loss = 6.7349\n",
      "step 250, loss = 6.2635\n",
      "step 300, loss = 6.0314\n",
      "step 350, loss = 5.5470\n",
      "step 400, loss = 6.1993\n",
      "step 450, loss = 5.9281\n",
      "step 500, loss = 6.2200\n",
      "step 550, loss = 5.8801\n",
      "step 600, loss = 5.7898\n",
      "step 650, loss = 5.6240\n",
      "step 700, loss = 5.7769\n",
      "step 750, loss = 5.0353\n",
      "step 800, loss = 5.2699\n",
      "step 850, loss = 5.4781\n",
      "step 900, loss = 5.1874\n",
      "step 950, loss = 5.5779\n"
     ]
    }
   ],
   "source": [
    "### Training Loop ###\n",
    "model = LanguageModel(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(1000):  # number of training steps\n",
    "    # sample a random chunk of text\n",
    "    start = np.random.randint(0, len(token_ids) - config.max_seq_len - 1)\n",
    "    x_ids = torch.tensor(token_ids[start:start+config.max_seq_len])\n",
    "    y_ids = torch.tensor(token_ids[start+1:start+config.max_seq_len+1])\n",
    "    logits = model(x_ids)\n",
    "    targets = y_ids\n",
    "    loss = loss_fn(logits, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"step {step}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c182b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be a smart and the ladies to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be "
     ]
    }
   ],
   "source": [
    "max_num_tokens = 50\n",
    "prompt_text = \"You shall not\"\n",
    "\n",
    "for i in range(max_num_tokens):\n",
    "    prompt_tokens = [token_to_id[tok] for tok in prompt_text.lower().split()]\n",
    "    prompt_tensor = torch.tensor(prompt_tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(prompt_tensor)\n",
    "    \n",
    "    last_logits = logits[-1]\n",
    "    prob = torch.softmax(last_logits, dim=-1)\n",
    "    next_token_id = torch.argmax(prob).item()\n",
    "    next_token = id_to_token[next_token_id]\n",
    "    print(next_token, end=' ')\n",
    "\n",
    "    # append to prompt\n",
    "    prompt_text += \" \" + next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b53c9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Academie Brochu is a historic school at 29 Pine Street in Southbridge, Massachusetts. Built in 1899, it is one of the city's most imposing Colonial Revival buildings, and a significant element of the development of its Franco-American community. The building was listed on the National Register of Historic Places on June 22, 1989. It was gifted to Harrington Memorial Hospital and now houses Harrington Health System offices.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://en.wikipedia.org/api/rest_v1/page/random/summary\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"samsbaugh (samsbaugh@mines.edu)\"\n",
    "}\n",
    "\n",
    "resp = requests.get(url, headers=headers)\n",
    "#print(resp.status_code)\n",
    "#print(resp.json()[\"title\"])\n",
    "data = resp.json()\n",
    "\n",
    "#print(data[\"title\"])\n",
    "print(data[\"extract\"])\n",
    "#print(data[\"content_urls\"][\"desktop\"][\"page\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
